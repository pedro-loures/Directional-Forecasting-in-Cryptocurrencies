{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_datapath, test_datapath\n",
    "from utils import train_val_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "targets_for_test_df = pd.read_csv('data/targets_for_test.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_submission(test_df, filled_test_predictions, filename='submission.csv'):\n",
    "    # Create a new DataFrame for the submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': test_df['row_id'],\n",
    "        'target': [0] + filled_test_predictions\n",
    "    })\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sliding_windows_batch(df, window_size, batch_size):\n",
    "    for start in range(0, len(df) - window_size + 1, batch_size):\n",
    "        end = min(start + batch_size, len(df) - window_size + 1)\n",
    "        batch_windows = [\n",
    "            df.iloc[i:i + window_size].values for i in range(start, end)\n",
    "        ]\n",
    "        yield np.array(batch_windows)\n",
    "        \n",
    "def create_h5py_file(df, window_size, batch_size, filename):\n",
    "    with h5py.File(filename, 'w') as h5f:\n",
    "        batch_index = 0\n",
    "        for batch in create_sliding_windows_batch(df, window_size, batch_size=batch_size):\n",
    "            train_images = batch.reshape(-1, window_size, df.shape[1])\n",
    "            h5f.create_dataset(f'batch_{batch_index}', data=train_images)\n",
    "            batch_index += 1\n",
    "\n",
    "train_df = pd.read_csv(train_datapath)\n",
    "test_df = pd.read_csv(test_datapath)\n",
    "\n",
    "\n",
    "window_size = 60\n",
    "batch_size = 1024\n",
    "\n",
    "# split the train_df into train and val\n",
    "X_train, y_train, X_val, y_val = train_val_split(train_df)\n",
    "X_train['target'] = y_train\n",
    "train_slice_df = X_train.copy()\n",
    "\n",
    "X_val['target'] = y_val\n",
    "val_slice_df = X_val.copy()\n",
    "\n",
    "train_file = 'data/train_images.h5'\n",
    "# if not os.path.exists(train_file):\n",
    "    create_h5py_file(train_slice_df, window_size, batch_size, train_file)\n",
    "\n",
    "val_file = 'data/validation_images.h5'\n",
    "if not os.path.exists(val_file):\n",
    "    create_h5py_file(val_slice_df, window_size, batch_size, val_file)\n",
    "\n",
    "test_file = 'data/test_images.h5'\n",
    "if not os.path.exists(test_file):\n",
    "    create_h5py_file(test_df, window_size, batch_size, test_file)\n",
    "    \n",
    "del train_df, test_df, X_train, y_train, X_val, y_val, train_slice_df, val_slice_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(test_df, filled_test_predictions, filename='submission.csv'):\n",
    "    # Create a new DataFrame for the submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': test_df['row_id'],\n",
    "        'target': [0] + filled_test_predictions\n",
    "    })\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to {filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * (window_size // 2 // 2), 128)  # Adjust based on pooling layers\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * (window_size // 2 // 2))  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def load_data_for_prediction_batch(h5_file, start_index, batch_size, window_size):\n",
    "    with h5py.File(h5_file, 'r') as h5f:\n",
    "        inputs = h5f['inputs'][start_index:start_index + batch_size]  # Load a batch of windows\n",
    "    return inputs  # Shape will be (batch_size, window_size, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_datapath)\n",
    "\n",
    "\n",
    "\n",
    "# split the train_df into train and val\n",
    "X_train, y_train, X_val, y_val = train_val_split(train_df)\n",
    "X_train['target'] = y_train\n",
    "\n",
    "train_slice_df = X_train.copy()\n",
    "\n",
    "X_val['target'] = y_val\n",
    "val_slice_df = X_val.copy()\n",
    "\n",
    "\n",
    "input_channels = train_df.shape[1] - 1 # Columns minus the target\n",
    "num_classes = 1  # Binary classification\n",
    "window_size = 60\n",
    "batch_size = 1024\n",
    "train_targets = train_slice_df['target'].values\n",
    "\n",
    "del train_df, X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_slice_df, train_file, window_size, batch_size, num_epochs=10, learning_rate=0.001):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for start_index in range(0, len(train_targets) - window_size + 1, batch_size):\n",
    "            # Load a batch of data\n",
    "            inputs = load_data_for_prediction_batch(train_file, start_index, batch_size, window_size)\n",
    "            inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "            targets = torch.tensor(train_targets.values[start_index:start_index + batch_size], dtype=torch.float32)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize the model\n",
    "model = BasicCNN(input_channels, num_classes)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_slice_df, train_file, window_size, batch_size)\n",
    "\n",
    "train_df = pd.read_csv(train_datapath)\n",
    "X_train, y_train, X_val, y_val = train_val_split(train_df)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "\n",
    "\n",
    "# Evaluate the model on validation data in batches\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_targets = val_slice_df['target'].values\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start_index in range(0, len(val_targets) - window_size + 1, batch_size):\n",
    "        val_inputs = load_data_for_prediction_batch(val_file, start_index, batch_size, window_size)\n",
    "        val_inputs = torch.tensor(val_inputs, dtype=torch.float32)\n",
    "        val_outputs = model(val_inputs)\n",
    "        batch_predictions = (val_outputs.squeeze().numpy() > 0.5).astype(int)\n",
    "        val_predictions.extend(batch_predictions)\n",
    "\n",
    "y_pred = np.array(val_predictions[:len(val_targets)])\n",
    "\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n",
    "save_submission(X_val, y_pred, filename='submissionCNN.csv')\n",
    "\n",
    "del train_df, X_train, y_train, X_val, y_val, train_slice_df, val_slice_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
