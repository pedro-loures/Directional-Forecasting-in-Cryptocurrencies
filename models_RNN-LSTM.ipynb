{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_val_split\n",
    "from utils import train_datapath, test_datapath\n",
    "\n",
    "targets_for_test_df = pd.read_csv('data/targets_for_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create sequences\n",
    "def create_sequences(data, num_timesteps):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - num_timesteps + 1):\n",
    "        sequences.append(data[i:i+num_timesteps])\n",
    "    return np.array(sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_datapath)\n",
    "test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53061/53061 [==============================] - 172s 3ms/step - loss: 0.6924 - f1_score: 0.2087\n",
      "Epoch 2/10\n",
      "53061/53061 [==============================] - 171s 3ms/step - loss: 0.6924 - f1_score: 0.2636\n",
      "Epoch 3/10\n",
      "53061/53061 [==============================] - 184s 3ms/step - loss: 0.6927 - f1_score: 0.2897\n",
      "Epoch 4/10\n",
      "53061/53061 [==============================] - 184s 3ms/step - loss: 0.6927 - f1_score: 0.2892\n",
      "Epoch 5/10\n",
      "53061/53061 [==============================] - 194s 4ms/step - loss: 0.6926 - f1_score: 0.2970\n",
      "Epoch 6/10\n",
      "53061/53061 [==============================] - 210s 4ms/step - loss: 0.6927 - f1_score: 0.2947\n",
      "Epoch 7/10\n",
      "53061/53061 [==============================] - 221s 4ms/step - loss: 0.6927 - f1_score: 0.2983\n",
      "Epoch 8/10\n",
      "53061/53061 [==============================] - 204s 4ms/step - loss: 0.6927 - f1_score: 0.2957\n",
      "Epoch 9/10\n",
      "53061/53061 [==============================] - 186s 4ms/step - loss: 0.6928 - f1_score: 0.2841\n",
      "Epoch 10/10\n",
      "53061/53061 [==============================] - 184s 3ms/step - loss: 0.6928 - f1_score: 0.2888\n",
      "13265/13265 [==============================] - 27s 2ms/step\n",
      "Time elapsed: 32m 31.6s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.53143\n",
      "Validation F1 Macro Score: 0.36083\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Extract features and target\n",
    "X_train, y_train, X_val, y_val = train_val_split(train_df)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape data to fit RNN input requirements (samples, time steps, features)\n",
    "num_timesteps = 15\n",
    "num_features = X_train.shape[1]  # Assuming X_train has 10 features\n",
    "\n",
    "# Generate sequences\n",
    "X_train_seq = create_sequences(X_train, num_timesteps)\n",
    "X_val_seq = create_sequences(X_val, num_timesteps)\n",
    "\n",
    "# Adjust y_train and y_val accordingly\n",
    "y_train_seq = y_train[num_timesteps - 1:]\n",
    "y_val_seq = y_val[num_timesteps - 1:]\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_prob = model.predict(X_val_seq)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val_seq, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val_seq, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28426/28426 [==============================] - 59s 2ms/step\n",
      "Test Accuracy: 0.57593\n",
      "Test F1 Macro Score: 0.43709\n"
     ]
    }
   ],
   "source": [
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "X_test = test_df.drop(columns=['row_id'])\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape test data to fit RNN input requirements (samples, time steps, features)\n",
    "X_test_seq = create_sequences(X_test, num_timesteps)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions_prob = model.predict(X_test_seq)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "test_predictions = (test_predictions_prob > 0.5).astype('float32')\n",
    "\n",
    "# Fil the predictions to match the length of the targets_for_test_df\n",
    "filled_test_predictions = (len(targets_for_test_df) - len(test_predictions)) * [0]  + test_predictions.flatten().tolist()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, filled_test_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, filled_test_predictions, average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 15, 10), found shape=(None, 10)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m test_predictions_prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert probabilities to binary predictions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m (test_predictions_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileub_7tsjm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 15, 10), found shape=(None, 10)\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame for the submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'target': filled_test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Predictions saved to submission.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df, test_df, X_train, y_train, X_val, y_val, X_train_seq, X_val_seq, y_train_seq, y_val_seq, model, y_pred_prob, y_pred, X_test, X_test_seq, test_predictions_prob, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_train = pd.read_csv('data/svd_train.csv')\n",
    "svd_test = pd.read_csv('data/svd_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52919/52919 [==============================] - 178s 3ms/step - loss: 0.6905 - f1_score: 0.3922\n",
      "Epoch 2/10\n",
      "52919/52919 [==============================] - 178s 3ms/step - loss: 0.6887 - f1_score: 0.4011\n",
      "Epoch 3/10\n",
      "52919/52919 [==============================] - 174s 3ms/step - loss: 0.6881 - f1_score: 0.4064\n",
      "Epoch 4/10\n",
      "52919/52919 [==============================] - 174s 3ms/step - loss: 0.6874 - f1_score: 0.4103\n",
      "Epoch 5/10\n",
      "52919/52919 [==============================] - 175s 3ms/step - loss: 0.6874 - f1_score: 0.4114\n",
      "Epoch 6/10\n",
      "52919/52919 [==============================] - 175s 3ms/step - loss: 0.6874 - f1_score: 0.4158\n",
      "Epoch 7/10\n",
      "52919/52919 [==============================] - 176s 3ms/step - loss: 0.6873 - f1_score: 0.4157\n",
      "Epoch 8/10\n",
      "52919/52919 [==============================] - 177s 3ms/step - loss: 0.6871 - f1_score: 0.4153\n",
      "Epoch 9/10\n",
      "52919/52919 [==============================] - 175s 3ms/step - loss: 0.6871 - f1_score: 0.4152\n",
      "Epoch 10/10\n",
      "52919/52919 [==============================] - 176s 3ms/step - loss: 0.6870 - f1_score: 0.4156\n",
      "13230/13230 [==============================] - 27s 2ms/step\n",
      "Time elapsed: 29m 58.5s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.51924\n",
      "Validation F1 Macro Score: 0.50666\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Extract features and target\n",
    "X_train, y_train, X_val, y_val = train_val_split(svd_train)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape data to fit RNN input requirements (samples, time steps, features)\n",
    "num_timesteps = 15\n",
    "num_features = X_train.shape[1]  # Assuming X_train has 10 features\n",
    "\n",
    "# Generate sequences\n",
    "X_train_seq = create_sequences(X_train, num_timesteps)\n",
    "X_val_seq = create_sequences(X_val, num_timesteps)\n",
    "\n",
    "# Adjust y_train and y_val accordingly\n",
    "y_train_seq = y_train[num_timesteps - 1:]\n",
    "y_val_seq = y_val[num_timesteps - 1:]\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_prob = model.predict(X_val_seq)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val_seq, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val_seq, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28423/28423 [==============================] - 54s 2ms/step\n",
      "Test Accuracy: 0.46368\n",
      "Test F1 Macro Score: 0.45448\n"
     ]
    }
   ],
   "source": [
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "X_SVD_test = svd_test.drop(columns=['row_id'])\n",
    "X_SVD_test = scaler.transform(X_SVD_test)\n",
    "\n",
    "# Reshape test data to fit RNN input requirements (samples, time steps, features)\n",
    "X_SVD_test_seq = create_sequences(X_SVD_test, num_timesteps)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions_prob = model.predict(X_SVD_test_seq)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "test_predictions = (test_predictions_prob > 0.5).astype(int)\n",
    "\n",
    "filled_test_predictions = (len(targets_for_test_df) - len(test_predictions)) * [0]  + test_predictions.flatten().tolist()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, filled_test_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, filled_test_predictions, average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new DataFrame for the submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'target': filled_test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Predictions saved to submission.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del svd_train, svd_test, X_train, y_train, X_val, y_val, X_train_seq, X_val_seq, y_train_seq, y_val_seq, model, y_pred_prob, y_pred, X_test, X_test_seq, test_predictions_prob, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_train_df = pd.read_csv('data/treated_train.csv')\n",
    "treated_test_df = pd.read_csv('data/treated_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 284. MiB for an array with shape (1693399, 22) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[0;32m     14\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 15\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m X_val \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Reshape data to fit RNN input requirements (samples, time steps, features)\u001b[39;00m\n",
      "File \u001b[1;32md:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32md:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32md:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:999\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(X)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 999\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m \u001b[43m_incremental_mean_and_var\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples_seen_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mptp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1143\u001b[0m, in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     T \u001b[38;5;241m=\u001b[39m new_sum \u001b[38;5;241m/\u001b[39m new_sample_count\n\u001b[1;32m-> 1143\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;66;03m# equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;66;03m# safer because np.float64(X*W) != np.float64(X)*np.float64(W)\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m         correction \u001b[38;5;241m=\u001b[39m _safe_accumulator_op(\n\u001b[0;32m   1148\u001b[0m             np\u001b[38;5;241m.\u001b[39mmatmul, sample_weight, np\u001b[38;5;241m.\u001b[39mwhere(X_nan_mask, \u001b[38;5;241m0\u001b[39m, temp)\n\u001b[0;32m   1149\u001b[0m         )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 284. MiB for an array with shape (1693399, 22) and data type float64"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Extract features and target\n",
    "X_train, y_train, X_val, y_val = train_val_split(treated_train_df)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape data to fit RNN input requirements (samples, time steps, features)\n",
    "num_timesteps = 15\n",
    "num_features = X_train.shape[1]  # Assuming X_train has 10 features\n",
    "\n",
    "# Generate sequences\n",
    "X_train_seq = create_sequences(X_train, num_timesteps)\n",
    "X_val_seq = create_sequences(X_val, num_timesteps)\n",
    "\n",
    "# Adjust y_train and y_val accordingly\n",
    "y_train_seq = y_train[num_timesteps - 1:]\n",
    "y_val_seq = y_val[num_timesteps - 1:]\n",
    "\n",
    "# Define the RNN treated_model\n",
    "treated_model = Sequential()\n",
    "treated_model.add(SimpleRNN(50, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "treated_model.add(Dense(1, activation='sigmoid'))\n",
    "treated_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n",
    "\n",
    "# Train the treated_model\n",
    "treated_model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_prob = treated_model.predict(X_val_seq)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val_seq, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val_seq, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape test data to fit RNN input requirements (samples, time steps, features)\n",
    "X_test_seq = create_sequences(X_test, num_timesteps)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions_prob = model.predict(X_test_seq)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "test_predictions = (test_predictions_prob > 0.5).astype(int)\n",
    "\n",
    "filled_test_predictions = (len(targets_for_test_df) - len(test_predictions)) * [0]  + test_predictions.flatten().tolist()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, filled_test_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, filled_test_predictions, average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del treated_train_df, treated_test_df, X_train, y_train, X_val, y_val, X_train_seq, X_val_seq, y_train_seq, y_val_seq, treated_model, y_pred_prob, y_pred, X_test, X_test_seq, test_predictions_prob, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_train_df = pd.read_csv('data/new_features_train.csv')\n",
    "new_features_test_df = pd.read_csv('data/new_features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Extract features and target\n",
    "X_train, y_train, X_val, y_val = train_val_split(new_features_train_df)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape data to fit RNN input requirements (samples, time steps, features)\n",
    "num_timesteps = 15\n",
    "num_features = X_train.shape[1]  # Assuming X_train has 10 features\n",
    "\n",
    "# Generate sequences\n",
    "X_train_seq = create_sequences(X_train, num_timesteps)\n",
    "X_val_seq = create_sequences(X_val, num_timesteps)\n",
    "\n",
    "# Adjust y_train and y_val accordingly\n",
    "y_train_seq = y_train[num_timesteps - 1:]\n",
    "y_val_seq = y_val[num_timesteps - 1:]\n",
    "\n",
    "# Define the RNN only_model\n",
    "only_model = Sequential()\n",
    "only_model.add(SimpleRNN(50, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "only_model.add(Dense(1, activation='sigmoid'))\n",
    "only_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n",
    "\n",
    "# Train the only_model\n",
    "only_model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_prob = only_model.predict(X_val_seq)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val_seq, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val_seq, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "X_test = new_features_test_df.drop(columns=['row_id'])\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape test data to fit RNN input requirements (samples, time steps, features)\n",
    "X_test_seq = create_sequences(X_test, num_timesteps)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions_prob = model.predict(X_test_seq)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "test_predictions = (test_predictions_prob > 0.5).astype(int)\n",
    "\n",
    "filled_test_predictions = (len(targets_for_test_df) - len(test_predictions)) * [0]  + test_predictions.flatten().tolist()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, filled_test_predictions)\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, filled_test_predictions, average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
