{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_addons as tfa\n",
    "import time\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'evaluate_model' from 'utils' (d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_val_split\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_datapath, test_datapath\n\u001b[0;32m      5\u001b[0m targets_for_test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/targets_for_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'evaluate_model' from 'utils' (d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\utils.py)"
     ]
    }
   ],
   "source": [
    "from utils import train_val_split\n",
    "from utils import evaluate_model\n",
    "from utils import train_datapath, test_datapath\n",
    "\n",
    "targets_for_test_df = pd.read_csv('data/targets_for_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create sequences\n",
    "def create_sequences(data, num_timesteps):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - num_timesteps + 1):\n",
    "        sequences.append(data[i:i+num_timesteps])\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "def evaluate_model_performance(model, X_val_seq, y_val_seq):\n",
    "    # Predict probabilities\n",
    "    y_pred_prob = model.predict(X_val_seq)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    y_val = y_val_seq.copy()\n",
    "    filled_y_pred = y_pred.copy()\n",
    "    if len(y_val_seq) == (y_pred):\n",
    "        pass\n",
    "    elif len(y_val_seq) > len(y_pred):\n",
    "        difference = y_val_seq - y_pred\n",
    "        filled_y_pred = np.concatenate([np.zeros(difference), filled_y_pred])\n",
    "    else: \n",
    "        y_val = np.concatenate([0], y_val)\n",
    "        \n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val_seq, filled_y_pred)\n",
    "    print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "    # Calculate F1 macro score\n",
    "    f1_macro = f1_score(y_val_seq, filled_y_pred, average='macro')\n",
    "    print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n",
    "    return y_pred, y_pred_prob\n",
    "\n",
    "\n",
    "def save_submission(test_df, filled_test_predictions, filename='submission.csv'):\n",
    "    # Create a new DataFrame for the submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': test_df['row_id'],\n",
    "        'target': [0] + filled_test_predictions\n",
    "    })\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to {filename}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_rnn_model(train_df, num_timesteps=15, epochs=10, batch_size=32):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Extract features and target\n",
    "    X_train, y_train, X_val, y_val = train_val_split(train_df)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Reshape data to fit RNN input requirements (samples, time steps, features)\n",
    "    num_features = X_train.shape[1]  # Assuming X_train has 10 features\n",
    "\n",
    "    # Generate sequences\n",
    "    X_train_seq = create_sequences(X_train, num_timesteps)\n",
    "    X_val_seq = create_sequences(X_val, num_timesteps)\n",
    "\n",
    "    # Adjust y_train and y_val accordingly\n",
    "    y_train_seq = y_train[num_timesteps - 1:]\n",
    "    y_val_seq = y_val[num_timesteps - 1:]\n",
    "\n",
    "    # Define the RNN model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(50, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_seq, y_train_seq, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    minutes = (end_time - start_time) // 60\n",
    "    seconds = (end_time - start_time) % 60\n",
    "\n",
    "    print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "    print('--------------------------------------')\n",
    "\n",
    "    evaluate_model_performance(model, X_val_seq, y_val_seq)\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.90 GiB for an array with shape (1697936, 15, 10) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the training data and train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_datapath)\n\u001b[1;32m----> 3\u001b[0m model, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_df\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the test data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m, in \u001b[0;36mtrain_rnn_model\u001b[1;34m(train_df, num_timesteps, epochs, batch_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m num_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming X_train has 10 features\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Generate sequences\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m X_train_seq \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m X_val_seq \u001b[38;5;241m=\u001b[39m create_sequences(X_val, num_timesteps)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Adjust y_train and y_val accordingly\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(data, num_timesteps)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m num_timesteps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     sequences\u001b[38;5;241m.\u001b[39mappend(data[i:i\u001b[38;5;241m+\u001b[39mnum_timesteps])\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(sequences)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.90 GiB for an array with shape (1697936, 15, 10) and data type float64"
     ]
    }
   ],
   "source": [
    "# Load the training data and train the model\n",
    "train_df = pd.read_csv(train_datapath)\n",
    "model, scaler = train_rnn_model(train_df)\n",
    "del train_df\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv(test_datapath)\n",
    "y_pred, ypred_prob = evaluate_model_performance(model, scaler, test_df)\n",
    "\n",
    "# Save the submission\n",
    "save_submission(test_df, y_pred, 'crude_rnn_submission.csv')\n",
    "\n",
    "# Save predictions and probabilities\n",
    "predictions_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'target': y_pred,\n",
    "    'probability': ypred_prob\n",
    "})\n",
    "predictions_df.to_csv('crude_rnn_prob_predictions.csv', index=False)\n",
    "\n",
    "del model, scaler, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\env_rnn\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52919/52919 [==============================] - 211s 4ms/step - loss: 0.6892 - f1_score: 0.3942\n",
      "Epoch 2/10\n",
      "52919/52919 [==============================] - 204s 4ms/step - loss: 0.6890 - f1_score: 0.3940\n",
      "Epoch 3/10\n",
      "52919/52919 [==============================] - 202s 4ms/step - loss: 0.6884 - f1_score: 0.3939\n",
      "Epoch 4/10\n",
      "52919/52919 [==============================] - 205s 4ms/step - loss: 0.6893 - f1_score: 0.3828\n",
      "Epoch 5/10\n",
      "52919/52919 [==============================] - 206s 4ms/step - loss: 0.6901 - f1_score: 0.3850\n",
      "Epoch 6/10\n",
      "52919/52919 [==============================] - 201s 4ms/step - loss: 0.6903 - f1_score: 0.3854\n",
      "Epoch 7/10\n",
      "52919/52919 [==============================] - 207s 4ms/step - loss: 0.6897 - f1_score: 0.3914\n",
      "Epoch 8/10\n",
      "52919/52919 [==============================] - 191s 4ms/step - loss: 0.6882 - f1_score: 0.4003\n",
      "Epoch 9/10\n",
      "52919/52919 [==============================] - 178s 3ms/step - loss: 0.6881 - f1_score: 0.3995\n",
      "Epoch 10/10\n",
      "52919/52919 [==============================] - 177s 3ms/step - loss: 0.6880 - f1_score: 0.4052\n",
      "13230/13230 [==============================] - 26s 2ms/step\n",
      "Time elapsed: 33m 41.0s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.53228\n",
      "Validation F1 Macro Score: 0.41841\n"
     ]
    }
   ],
   "source": [
    "# Load the training data and train the model\n",
    "svd_train = pd.read_csv('data/svd_train.csv')\n",
    "model, scaler = train_rnn_model(svd_train)\n",
    "del svd_train\n",
    "\n",
    "# Load the test data\n",
    "svd_test = pd.read_csv('data/svd_test.csv')\n",
    "y_pred, ypred_prob = evaluate_model_performance(model, scaler, svd_test)\n",
    "\n",
    "# Save the submission\n",
    "save_submission(test_df, y_pred, 'svd_rnn_submission.csv')\n",
    "\n",
    "\n",
    "# Save predictions and probabilities\n",
    "predictions_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'target': y_pred,\n",
    "    'probability': ypred_prob\n",
    "})\n",
    "predictions_df.to_csv('crude_rnn_prob_predictions.csv', index=False)\n",
    "\n",
    "del model, scaler, svd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data and train the model\n",
    "new_features_train_df = pd.read_csv('data/new_features_train.csv')\n",
    "\n",
    "model, scaler = train_rnn_model(new_features_train_df)\n",
    "del new_features_train_df\n",
    "\n",
    "# Load the test data\n",
    "new_features_test_df = pd.read_csv('data/new_features_test.csv')\n",
    "y_pred, ypred_prob = evaluate_model_performance(model, scaler, new_features_test_df)\n",
    "\n",
    "# Save the submission\n",
    "save_submission(test_df, y_pred, 'only_new_rnn_submission.csv')\n",
    "\n",
    "\n",
    "# Save predictions and probabilities\n",
    "predictions_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'target': y_pred,\n",
    "    'probability': ypred_prob\n",
    "})\n",
    "predictions_df.to_csv('crude_rnn_prob_predictions.csv', index=False)\n",
    "\n",
    "del model, scaler, new_features_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data and train the model\n",
    "treated_train_df = pd.read_csv('data/treated_train.csv')\n",
    "\n",
    "model, scaler = train_rnn_model(new_features_train_df)\n",
    "del svd_train\n",
    "\n",
    "# Load the test data\n",
    "treated_test_df = pd.read_csv('data/treated_test.csv')\n",
    "y_pred, ypred_prob = evaluate_model_performance(model, scaler, new_features_test_df)\n",
    "\n",
    "# Save the submission\n",
    "save_submission(test_df, y_pred, 'new_feat_rnn_submission.csv')\n",
    "\n",
    "\n",
    "# Save predictions and probabilities\n",
    "predictions_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'target': y_pred,\n",
    "    'probability': ypred_prob\n",
    "})\n",
    "predictions_df.to_csv('crude_rnn_prob_predictions.csv', index=False)\n",
    "\n",
    "del model, scaler, svd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(train_df, num_timesteps=15, epochs=10, batch_size=32):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Extract features and target\n",
    "    X_train, y_train, X_val, y_val = train_val_split(train_df)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Reshape data to fit LSTM input requirements (samples, time steps, features)\n",
    "    num_features = X_train.shape[1]  # Assuming X_train has 10 features\n",
    "\n",
    "    # Generate sequences\n",
    "    X_train_seq = create_sequences(X_train, num_timesteps)\n",
    "    X_val_seq = create_sequences(X_val, num_timesteps)\n",
    "\n",
    "    # Adjust y_train and y_val accordingly\n",
    "    y_train_seq = y_train[num_timesteps - 1:]\n",
    "    y_val_seq = y_val[num_timesteps - 1:]\n",
    "\n",
    "    # Define the LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(50, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "    lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tfa.metrics.F1Score(num_classes=1, threshold=0.5)])\n",
    "\n",
    "    # Train the model\n",
    "    lstm_model.fit(X_train_seq, y_train_seq, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    end_time = time.time()\n",
    "    minutes = (end_time - start_time) // 60\n",
    "    seconds = (end_time - start_time) % 60\n",
    "    print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "    print('--------------------------------------')\n",
    "\n",
    "    evaluate_model_performance(model, X_val_seq, y_val_seq)\n",
    "    \n",
    "    return lstm_model, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.90 GiB for an array with shape (1697936, 15, 10) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the training data and train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_datapath)\n\u001b[1;32m----> 3\u001b[0m model, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_df\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the test data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mtrain_lstm_model\u001b[1;34m(train_df, num_timesteps, epochs, batch_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m num_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming X_train has 10 features\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Generate sequences\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m X_train_seq \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m X_val_seq \u001b[38;5;241m=\u001b[39m create_sequences(X_val, num_timesteps)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Adjust y_train and y_val accordingly\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(data, num_timesteps)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m num_timesteps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     sequences\u001b[38;5;241m.\u001b[39mappend(data[i:i\u001b[38;5;241m+\u001b[39mnum_timesteps])\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(sequences)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.90 GiB for an array with shape (1697936, 15, 10) and data type float64"
     ]
    }
   ],
   "source": [
    "# Load the training data and train the model\n",
    "train_df = pd.read_csv(train_datapath)\n",
    "model, scaler = train_lstm_model(train_df)\n",
    "del train_df\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv(test_datapath)\n",
    "y_pred, ypred_prob = evaluate_model_performance(model, scaler, test_df)\n",
    "\n",
    "# Save the submission\n",
    "save_submission(test_df, y_pred, 'crude_rnn_submission.csv')\n",
    "\n",
    "\n",
    "# Save predictions and probabilities\n",
    "predictions_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'target': y_pred,\n",
    "    'probability': ypred_prob\n",
    "})\n",
    "predictions_df.to_csv('crude_rnn_predictions.csv', index=False)\n",
    "\n",
    "del model, scaler, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data and train the model\n",
    "svd_train = pd.read_csv('data/svd_train.csv')\n",
    "model, scaler = train_lstm_model(svd_train)\n",
    "del svd_train\n",
    "\n",
    "# Load the test data\n",
    "svd_test = pd.read_csv('data/svd_test.csv')\n",
    "y_pred, ypred_prob = evaluate_model_performance(model, scaler, svd_test)\n",
    "\n",
    "# Save the submission\n",
    "save_submission(svd_test, y_pred, 'crude_rnn_submission.csv')\n",
    "\n",
    "\n",
    "# Save predictions and probabilities\n",
    "predictions_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'target': y_pred,\n",
    "    'probability': ypred_prob\n",
    "})\n",
    "predictions_df.to_csv('crude_rnn_predictions.csv', index=False)\n",
    "\n",
    "del model, scaler, svd_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
