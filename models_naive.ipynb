{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (utils.py, line 43)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\n\u001b[1;33m    from utils import train_val_split\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32md:\\01_GitHub\\Directional-Forecasting-in-Cryptocurrencies\\utils.py:43\u001b[1;36m\u001b[0m\n\u001b[1;33m    accuracy = accuracy_score(y_val, filled_y_pred)k\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from utils import train_val_split\n",
    "from utils import train_datapath, test_datapath\n",
    "\n",
    "targets_for_test_df = pd.read_csv('data/targets_for_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.531152824108102\n",
      "Validation F1 Macro Score: 0.3468973284345402\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Check if there are more Ones or Zeroes on train_df[target]\n",
    "majority_class = train_df['target'].value_counts().idxmax()\n",
    "\n",
    "y_pred = [majority_class] * len(y_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.58005\n",
      "Test F1 Macro Score: 0.36711\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_prediction = [majority_class] * len(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_prediction[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_prediction[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Random): 0.50025\n",
      "Test F1 Macro Score (Random): 0.49700\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random predictions for the test data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "random_predictions = np.random.choice([0, 1], size=len(test_df))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, random_predictions[:len(targets_for_test_df)])\n",
    "print(f'Test Accuracy (Random): {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, random_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score (Random): {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a new DataFrame for the submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'target': random_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Predictions saved to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (shifted): 0.50648\n",
      "Test F1 Macro Score (shifted): 0.49350\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate random predictions for the test data\n",
    "shift_prediction = targets_for_test_df.shift(1).fillna(0)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, shift_prediction[:len(targets_for_test_df)])\n",
    "print(f'Test Accuracy (shifted): {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, shift_prediction[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score (shifted): {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random with seasonal trend as probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minute of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_datapath)\n",
    "test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create copies of train_df and test_df\n",
    "train_df_copy = train_df.copy()\n",
    "test_df_copy = test_df.copy()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "train_df_copy['datetime'] = pd.to_datetime(train_df_copy['timestamp'], unit='s')\n",
    "test_df_copy['datetime'] = pd.to_datetime(test_df_copy['timestamp'], unit='s')\n",
    "\n",
    "test_df_copy['time'] = test_df_copy['datetime'].dt.time\n",
    "train_df_copy['time'] = train_df_copy['datetime'].dt.time\n",
    "\n",
    "# Group by minute of the day and calculate mean for each group\n",
    "train_grouped_by_minute = train_df_copy.groupby('time').mean()\n",
    "test_grouped_by_minute = test_df_copy.groupby('time').mean()\n",
    "labels = train_grouped_by_minute.index\n",
    "\n",
    "# Group by minute_of_day and calculate the mean target value for each group\n",
    "mean_target_by_minute = train_grouped_by_minute['target']\n",
    "mean_target_by_minute.index = labels.astype(str)\n",
    "mean_target_by_minute.to_csv('data/mean_target_by_minute.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minute_seasonality = []\n",
    "predictions = []\n",
    "for idx, hour in enumerate(test_df_copy['time'].astype(str)):\n",
    "    prob1 = mean_target_by_minute.loc[hour]\n",
    "    prob0 = 1 - prob1\n",
    "    minute_seasonality.append(prob1)\n",
    "    predictions.append(np.random.choice([0, 1], p=[prob0, prob1]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.50365\n",
      "Validation F1 Macro Score: 0.49820\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, predictions[1:])\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, predictions[1:], average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hour of week seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2022-05-17 19:59:00', '2022-05-17 20:00:00',\n",
       "               '2022-05-17 20:01:00', '2022-05-17 20:02:00',\n",
       "               '2022-05-17 20:03:00', '2022-05-17 20:04:00',\n",
       "               '2022-05-17 20:05:00', '2022-05-17 20:06:00',\n",
       "               '2022-05-17 20:07:00', '2022-05-17 20:08:00',\n",
       "               ...\n",
       "               '2024-02-08 12:05:00', '2024-02-08 12:06:00',\n",
       "               '2024-02-08 12:07:00', '2024-02-08 12:08:00',\n",
       "               '2024-02-08 12:09:00', '2024-02-08 12:10:00',\n",
       "               '2024-02-08 12:11:00', '2024-02-08 12:12:00',\n",
       "               '2024-02-08 12:13:00', '2024-02-08 12:14:00'],\n",
       "              dtype='datetime64[ns]', name='timestamp', length=909616, freq=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get hour from the index of test_target_3h_df\n",
    "pd.to_datetime(targets_for_test_df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data to 3-hour intervals and sum the target\n",
    "target_3h_df = train_df_copy.copy()\n",
    "target_3h_df.set_index('datetime', inplace=True)\n",
    "target_3h_df['3h_group'] = target_3h_df.index.to_series().dt.floor('3h').dt.hour\n",
    "\n",
    "\n",
    "# Add columns for day of the week and hour of the day\n",
    "target_3h_df['day_of_week'] = target_3h_df.index.dayofweek\n",
    "target_3h_df['hour'] = target_3h_df.index.hour\n",
    "target_3h_df['week_day_hour'] = target_3h_df['day_of_week'].astype(str).str.zfill(2) + '-' + target_3h_df['3h_group'].astype(str).str.zfill(2) + ':00'  \n",
    "grouped_target_3h = target_3h_df.groupby('week_day_hour')['target'].mean()\n",
    "target_3h_df.to_csv('data/target_3h_df.csv')\n",
    "\n",
    "# Resample the data to 3-hour intervals and sum the target\n",
    "test_target_3h_df = pd.DataFrame()\n",
    "test_target_3h_df['target'] = targets_for_test_df.copy()\n",
    "test_target_3h_df['datetime'] = pd.to_datetime(test_target_3h_df.index)\n",
    "test_target_3h_df.set_index('datetime', inplace=True)\n",
    "\n",
    "test_target_3h_df['hour'] = test_target_3h_df.index.hour\n",
    "test_target_3h_df['3h_group'] = test_target_3h_df.index.to_series().dt.floor('3h').dt.hour\n",
    "\n",
    "# Add columns for day of the week and hour of the day\n",
    "test_target_3h_df['day_of_week'] = test_target_3h_df.index.dayofweek\n",
    "test_target_3h_df['week_day_hour'] = test_target_3h_df['day_of_week'].astype(str).str.zfill(2) + '-' + test_target_3h_df['3h_group'].astype(str).str.zfill(2) + ':00'  \n",
    "\n",
    "test_target_3h_df.to_csv('data/test_target_3h_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (shifted): 0.50346\n",
      "Test F1 Macro Score (shifted): 0.49796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weekly_seasonality = []\n",
    "seasonal_predictions = []\n",
    "for idx, hour in enumerate(test_target_3h_df['week_day_hour']):\n",
    "    prob1 = grouped_target_3h.loc[hour]\n",
    "    prob0 = 1 - prob1\n",
    "    \n",
    "    weekly_seasonality.append(prob1)\n",
    "    seasonal_predictions.append(int(np.random.choice([0, 1], p=[prob0, prob1])))\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, seasonal_predictions)\n",
    "print(f'Test Accuracy (shifted): {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, seasonal_predictions, average='macro')\n",
    "print(f'Test F1 Macro Score (shifted): {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensamble them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (shifted): 0.50368\n",
      "Test F1 Macro Score (shifted): 0.49822\n"
     ]
    }
   ],
   "source": [
    "# just so we see what are the variables \n",
    "minute_seasonality = minute_seasonality\n",
    "weekly_seasonality = weekly_seasonality\n",
    "\n",
    "seasonal_predictions2 = []\n",
    "\n",
    "for i in range(len(targets_for_test_df)):\n",
    "    mean_probability1 = np.array(minute_seasonality[i], weekly_seasonality[i]).mean()\n",
    "    mean_probability0 = 1 - mean_probability1\n",
    "    seasonal_predictions2.append(np.random.choice([0, 1], p=[mean_probability0, mean_probability1]))\n",
    "    \n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, seasonal_predictions2)\n",
    "print(f'Test Accuracy (shifted): {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, seasonal_predictions2, average='macro')\n",
    "print(f'Test F1 Macro Score (shifted): {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(seasonal_predictions2).to_csv('submission/mean_seasonal_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
