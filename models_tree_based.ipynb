{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    " # Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import  train_val_split\n",
    "from utils import  train_datapath, test_datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3204058389.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 27\u001b[1;36m\u001b[0m\n\u001b[1;33m    accuracy = accuracy_score(y_val, filled_y_pred)k\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "targets_for_test_df = pd.read_csv('data/targets_for_test.csv', index_col=0)['close']\n",
    "\n",
    "\n",
    "def evaluate_model_performance(model, X_val_seq, y_val_seq):\n",
    "    # Predict probabilities\n",
    "    y_pred_prob = model.predict(X_val_seq)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    y_val = y_val_seq.copy()\n",
    "    filled_y_pred = y_pred.copy()\n",
    "    if len(y_val_seq) == len(y_pred):       # If the lengths are the same, do nothing\n",
    "        print('Lengths are the same')\n",
    "        pass\n",
    "    elif len(y_val_seq) > len(y_pred):      # If the target is longer than the prediction\n",
    "        print('Target is longer than prediction')\n",
    "        difference = len(y_val_seq) - len(y_pred)\n",
    "        filled_y_pred = np.concatenate([np.zeros(difference), filled_y_pred])\n",
    "    else:                                   # If the prediction is longer than the target \n",
    "        print('Prediction is longer than target')\n",
    "        y_val = np.concatenate([np.zeros(1), y_val])\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, filled_y_pred)k\n",
    "    print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "    # Calculate F1 macro score\n",
    "    f1_macro = f1_score(y_val, filled_y_pred, average='macro')\n",
    "    print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n",
    "    return y_pred, y_pred_prob\n",
    "\n",
    "\n",
    "def save_submission(test_df, filled_test_predictions, filename='submission.csv'):\n",
    "    filename = 'submissions/' + filename\n",
    "    # Create a new DataFrame for the submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': test_df['row_id'],\n",
    "        'target': [0] + filled_test_predictions\n",
    "    })\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to {filename}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_datapath)\n",
    "test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths are the same\n",
      "Validation Accuracy: 0.50210\n",
      "Validation F1 Macro Score: 0.50204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the decision tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the validation set\n",
    "# y_pred = model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "\n",
    "y_pred, ypred = evaluate_model_performance(model, X_val, y_val)\n",
    "\n",
    "# print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "# print('--------------------------------------')\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# # Calculate F1 macro score\n",
    "# f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "# print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is longer than target\n",
      "Validation Accuracy: 0.49465\n",
      "Validation F1 Macro Score: 0.49460\n",
      "Predictions saved to crude_dt_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred, ypred = evaluate_model_performance(model, X_test, targets_for_test_df)\n",
    "\n",
    "# Save the submission\n",
    "save_submission(test_df, y_pred, 'crude_dt_submission.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVDd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_train_df = pd.read_csv('data/svd_train.csv')\n",
    "svd_test_df = pd.read_csv('data/svd_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 1m 56.0s\n",
      "--------------------------------------\n",
      "Lengths are the same\n",
      "Validation Accuracy: 0.49714\n",
      "Validation F1 Macro Score: 0.49672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 1, ..., 0, 1, 1]), array([0., 0., 1., ..., 0., 1., 1.]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = svd_train_df.drop(columns=['target'])\n",
    "y = svd_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the decision tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "\n",
    "# # Predict on the validation set\n",
    "# y_pred = model.predict(X_val)\n",
    "\n",
    "evaluate_model_performance(model, X_val, y_val)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# # Calculate F1 macro score\n",
    "# f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "# print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909529, 12)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target is longer than prediction\n",
      "Validation Accuracy: 0.48812\n",
      "Validation F1 Macro Score: 0.48473\n",
      "Predictions saved to svd_dt_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = svd_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the submission\n",
    "# save_submission(test_df, y_pred, 'crude_rnn_submission.csv')\n",
    "\n",
    "\n",
    "y_pred, ypred = evaluate_model_performance(model, X_test, targets_for_test_df)\n",
    "save_submission(svd_test_df, y_pred, 'svd_dt_submission.csv')\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)])\t\n",
    "# print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# # Calculate F1 macro score\n",
    "# f1_macro = f1_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "# print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_train_df = pd.read_csv(train_datapath)\n",
    "treated_test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 2m 0.1s\n",
      "--------------------------------------\n",
      "Lengths are the same\n",
      "Validation Accuracy: 0.50210\n",
      "Validation F1 Macro Score: 0.50204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = treated_train_df.drop(columns=['target'])\n",
    "y = treated_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the decision tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "# y_pred = model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "\n",
    "\n",
    "y_pred, ypred = evaluate_model_performance(model, X_val, y_val)\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# # Calculate F1 macro score\n",
    "# f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "# print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is longer than target\n",
      "Validation Accuracy: 0.49465\n",
      "Validation F1 Macro Score: 0.49460\n",
      "Predictions saved to treated_dt_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred, ypred = evaluate_model_performance(model, X_test, targets_for_test_df)\n",
    "save_submission(treated_test_df, y_pred, 'treated_dt_submission.csv')\n",
    "\n",
    "\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)])\t\n",
    "# print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# # Calculate F1 macro score\n",
    "# f1_macro = f1_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "# print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del treated_train_df\n",
    "del treated_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_train_df = pd.read_csv('data/new_features_train.csv')\n",
    "new_features_test_df = pd.read_csv('data/new_features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 2m 33.6s\n",
      "--------------------------------------\n",
      "Lengths are the same\n",
      "Validation Accuracy: 0.50330\n",
      "Validation F1 Macro Score: 0.50142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, ..., 1, 0, 1]), array([1., 0., 0., ..., 1., 0., 1.]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = new_features_train_df.drop(columns=['target'])\n",
    "y = new_features_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the decision tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the validation set\n",
    "# y_pred = model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "\n",
    "evaluate_model_performance(model, X_val, y_val)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "# print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# # Calculate F1 macro score\n",
    "# f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "# print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target is longer than prediction\n",
      "Validation Accuracy: 0.49609\n",
      "Validation F1 Macro Score: 0.49069\n",
      "Predictions saved to only_nf_dt_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = new_features_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the submission\n",
    "y_prob, y_pred = evaluate_model_performance(model, X_test, targets_for_test_df)\n",
    "save_submission(new_features_test_df, y_pred, 'only_nf_dt_submission.csv')\n",
    "\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)])\t\n",
    "# print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# # Calculate F1 macro score\n",
    "# f1_macro = f1_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "# print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_features_train_df\n",
    "del new_features_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_datapath)\n",
    "test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Create and train the Random Forest model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Predict on the validation set\u001b[39;00m\n\u001b[0;32m     20\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.45986\n",
      "Test F1 Macro Score: 0.44456\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_train_df = pd.read_csv('data/train.csv')\n",
    "treated_test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 47m 59.0s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.51910\n",
      "Validation F1 Macro Score: 0.46335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = treated_train_df.drop(columns=['target'])\n",
    "y = treated_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [909616, 909529]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets_for_test_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_predictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtargets_for_test_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\t\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate F1 macro score\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [909616, 909529]"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del treated_train_df\n",
    "del treated_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_train_df = pd.read_csv('data/train.csv')\n",
    "new_features_test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_features_train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'target' is the column to predict and the rest are features\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnew_features_train_df\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m new_features_train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Split the data into training and validation sets\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_features_train_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = new_features_train_df.drop(columns=['target'])\n",
    "y = new_features_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = new_features_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_features_train_df   \n",
    "del new_features_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_datapath)\n",
    "test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [19:09:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0m 12.9s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.52729\n",
      "Validation F1 Macro Score: 0.44318\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.55726\n",
      "Test F1 Macro Score: 0.44548\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_train_df = pd.read_csv('data/train.csv')\n",
    "treated_test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [19:09:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0m 19.5s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.53091\n",
      "Validation F1 Macro Score: 0.48096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = treated_train_df.drop(columns=['target'])\n",
    "y = treated_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.56668\n",
      "Test F1 Macro Score: 0.44502\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del treated_train_df\n",
    "del treated_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_train_df = pd.read_csv(train_datapath)\n",
    "new_features_test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [04:38:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0m 9.6s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.51326\n",
      "Validation F1 Macro Score: 0.48311\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = new_features_train_df.drop(columns=['target'])\n",
    "y = new_features_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.46491\n",
      "Test F1 Macro Score: 0.46179\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = new_features_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_features_train_df\n",
    "del new_features_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainble Boost Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_datapath)\n",
    "test_df = pd.read_csv(test_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 17m 40.5s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.52687\n",
      "Validation F1 Macro Score: 0.42337\n"
     ]
    }
   ],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the Explainable Boosting Classifier model\n",
    "ebm_model = ExplainableBoostingClassifier(random_state=42)\n",
    "ebm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = ebm_model.predict(X_val)\n",
    "float_y_pred = y_pred.astype(float)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, float_y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, float_y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.55684\n",
      "Test F1 Macro Score: 0.46666\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = ebm_model.predict(X_test)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, float_test_prediction[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, float_test_prediction[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ebm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minterpret\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Show the EBM explanation\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m ebm_global \u001b[38;5;241m=\u001b[39m \u001b[43mebm_model\u001b[49m\u001b[38;5;241m.\u001b[39mexplain_global()\n\u001b[0;32m      6\u001b[0m show(ebm_global)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Show local explanations\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ebm_model' is not defined"
     ]
    }
   ],
   "source": [
    "from interpret import show\n",
    "\n",
    "\n",
    "# Show the EBM explanation\n",
    "ebm_global = ebm_model.explain_global()\n",
    "show(ebm_global)\n",
    "# Show local explanations\n",
    "ebm_local = ebm_model.explain_local(X_val, y_val)\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_train_df = pd.read_csv('data/train.csv')\n",
    "treated_test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\interpret\\glassbox\\_ebm\\_ebm.py:751: UserWarning: Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 15m 6.6s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.52670\n",
      "Validation F1 Macro Score: 0.42182\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = treated_train_df.drop(columns=['target'])\n",
    "y = treated_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "ebm_model = ExplainableBoostingClassifier(random_state=42)\n",
    "ebm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = ebm_model.predict(X_val)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, float_y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, float_y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.58042\n",
      "Test F1 Macro Score: 0.39537\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = ebm_model.predict(X_test)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, float_test_prediction[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, float_test_prediction[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2298853685648/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2298853685648/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2297576349392/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2297576349392/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the EBM explanation\n",
    "ebm_global = ebm_model.explain_global()\n",
    "show(ebm_global)\n",
    "# Show local explanations\n",
    "ebm_local = ebm_model.explain_local(X_val, y_val)\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del treated_train_df\n",
    "del treated_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_train_df = pd.read_csv('data/new_features_train.csv')\n",
    "new_features_test_df = pd.read_csv('data/new_features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Predict on the validation set\u001b[39;00m\n\u001b[0;32m     21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m ebm_model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m---> 22\u001b[0m float_test_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mtest_predictions\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     24\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     25\u001b[0m minutes \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = new_features_train_df.drop(columns=['target'])\n",
    "y = new_features_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "ebm_model = ExplainableBoostingClassifier(random_state=42)\n",
    "ebm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = ebm_model.predict(X_val)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 13m 33.5s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.52798\n",
      "Validation F1 Macro Score: 0.47661\n"
     ]
    }
   ],
   "source": [
    "test_predictions = ebm_model.predict(X_val)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, float_test_prediction)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, float_test_prediction, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.53023\n",
      "Test F1 Macro Score: 0.48283\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = new_features_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = ebm_model.predict(X_test)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df[:len(float_test_prediction)], float_test_prediction[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df[:len(float_test_prediction)], float_test_prediction[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2434487458704/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2434487458704/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2436041335632/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2436041335632/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the EBM explanation\n",
    "ebm_global = ebm_model.explain_global()\n",
    "show(ebm_global)\n",
    "# Show local explanations\n",
    "ebm_local = ebm_model.explain_local(X_val, y_val)\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_features_train_df\n",
    "del new_features_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
