{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    " # Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_treatment import train_df, test_df, \\\n",
    "                            treated_train_df, treated_test_df, \\\n",
    "                            new_features_train_df, new_features_test_df, \\\n",
    "                            targets_for_test_df\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 3m 16.5s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.50210\n",
      "Validation F1 Macro Score: 0.50204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the decision tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.48068\n",
      "Test F1 Macro Score: 0.48063\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 19m 12.6s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.50839\n",
      "Validation F1 Macro Score: 0.50081\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = treated_train_df.drop(columns=['target'])\n",
    "y = treated_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the decision tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.49774\n",
      "Test F1 Macro Score: 0.49437\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 8m 31.6s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.50330\n",
      "Validation F1 Macro Score: 0.50142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = new_features_train_df.drop(columns=['target'])\n",
    "y = new_features_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the decision tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.50254\n",
      "Test F1 Macro Score: 0.49721\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = new_features_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 40m 31.3s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.52253\n",
      "Validation F1 Macro Score: 0.40895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.45986\n",
      "Test F1 Macro Score: 0.44456\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 47m 59.0s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.51910\n",
      "Validation F1 Macro Score: 0.46335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = treated_train_df.drop(columns=['target'])\n",
    "y = treated_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [909616, 909529]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets_for_test_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_predictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtargets_for_test_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\t\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate F1 macro score\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [909616, 909529]"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_features_train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'target' is the column to predict and the rest are features\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnew_features_train_df\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m new_features_train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Split the data into training and validation sets\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_features_train_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = new_features_train_df.drop(columns=['target'])\n",
    "y = new_features_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = new_features_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df[:len(test_predictions)], test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [19:09:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0m 12.9s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.52729\n",
      "Validation F1 Macro Score: 0.44318\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.55726\n",
      "Test F1 Macro Score: 0.44548\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:158: UserWarning: [19:09:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0m 19.5s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.53091\n",
      "Validation F1 Macro Score: 0.48096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = treated_train_df.drop(columns=['target'])\n",
    "y = treated_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.56668\n",
      "Test F1 Macro Score: 0.44502\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'target' is the column to predict and the rest are features\u001b[39;00m\n",
      "\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m new_features_train_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = new_features_train_df.drop(columns=['target'])\n",
    "y = new_features_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = new_features_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainble Boost Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 14m 15.0s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.00000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate F1 macro score\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m f1_macro \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation F1 Macro Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_macro\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1293\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1114\u001b[0m     {\n\u001b[0;32m   1115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1140\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1141\u001b[0m ):\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1485\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1306\u001b[0m     {\n\u001b[0;32m   1307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1335\u001b[0m ):\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;124;03m    np.float64(0.12...)\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1485\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1789\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m \n\u001b[0;32m   1628\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m _check_zero_division(zero_division)\n\u001b[1;32m-> 1789\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1564\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1561\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m-> 1564\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m \u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m   1565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:114\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMix of label input types (string and number)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28msorted\u001b[39m(ys_labels))\n",
      "\u001b[1;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = train_df.drop(columns=['target'])\n",
    "y = train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the Explainable Boosting Classifier model\n",
    "ebm_model = ExplainableBoostingClassifier(random_state=42)\n",
    "ebm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = ebm_model.predict(X_val)\n",
    "float_y_pred = y_pred.astype(float)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, float_y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, float_y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.57972\n",
      "Test F1 Macro Score: 0.37788\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = ebm_model.predict(X_test)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, float_test_prediction[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, float_test_prediction[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2299954946512/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2299954946512/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2295824391504/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2295824391504/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interpret import show\n",
    "\n",
    "\n",
    "# Show the EBM explanation\n",
    "ebm_global = ebm_model.explain_global()\n",
    "show(ebm_global)\n",
    "# Show local explanations\n",
    "ebm_local = ebm_model.explain_local(X_val, y_val)\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\interpret\\glassbox\\_ebm\\_ebm.py:751: UserWarning: Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 15m 6.6s\n",
      "--------------------------------------\n",
      "Validation Accuracy: 0.52670\n",
      "Validation F1 Macro Score: 0.42182\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = treated_train_df.drop(columns=['target'])\n",
    "y = treated_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "ebm_model = ExplainableBoostingClassifier(random_state=42)\n",
    "ebm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = ebm_model.predict(X_val)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, float_y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, float_y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.58042\n",
      "Test F1 Macro Score: 0.39537\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = treated_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = ebm_model.predict(X_test)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, float_test_prediction[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, float_test_prediction[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2298853685648/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2298853685648/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2297576349392/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2297576349392/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the EBM explanation\n",
    "ebm_global = ebm_model.explain_global()\n",
    "show(ebm_global)\n",
    "# Show local explanations\n",
    "ebm_local = ebm_model.explain_local(X_val, y_val)\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'target' is the column to predict and the rest are features\u001b[39;00m\n",
      "\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m new_features_train_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Assuming 'target' is the column to predict and the rest are features\n",
    "X = new_features_train_df.drop(columns=['target'])\n",
    "y = new_features_train_df['target']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train = X.iloc[:len(X)*4//5]\n",
    "y_train = y.iloc[:len(y)*4//5]\n",
    "X_val= X.iloc[len(X)*4//5:]\n",
    "y_val = y.iloc[len(y)*4//5:]\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "ebm_model = ExplainableBoostingClassifier(random_state=42)\n",
    "ebm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = ebm_model.predict(X_val)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "end_time = time.time()\n",
    "minutes = (end_time - start_time) // 60\n",
    "seconds = (end_time - start_time) % 60\n",
    "print(f'Time elapsed: {minutes:.0f}m {seconds:.1f}s')\n",
    "print('--------------------------------------')\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "print(f'Validation F1 Macro Score: {f1_macro:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the X_test predictions using the targets_for_test_df\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "\n",
    "X_test = new_features_test_df.drop(columns=['row_id'])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = ebm_model.predict(X_test)\n",
    "float_test_prediction = test_predictions.astype(float)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)])\t\n",
    "print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "# Calculate F1 macro score\n",
    "f1_macro = f1_score(targets_for_test_df, test_predictions[:len(targets_for_test_df)], average='macro')\n",
    "print(f'Test F1 Macro Score: {f1_macro:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Show the EBM explanation\n",
    "ebm_global = ebm_model.explain_global()\n",
    "show(ebm_global)\n",
    "# Show local explanations\n",
    "ebm_local = ebm_model.explain_local(X_val, y_val)\n",
    "show(ebm_local)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
